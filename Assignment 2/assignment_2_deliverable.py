# -*- coding: utf-8 -*-
"""Assignment-2 Deliverable.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qrmmnEbMIvn-KuS97QEMHbxNVag-mugB
"""

a=[34,87,892,920,974,893]
maximum=max(a)
print(maximum)
minimum=min(a)
print(minimum)
sum=sum(a)
length=len(a)
average=sum/length
print(average)

a=[9,56,826,784]
multiplied_list = [element * 5 for element in a]
print(multiplied_list)

import numpy as np
x = np.random.randn(1000)
print("Average of the array elements:")
mean = x.mean()
print(mean)
print("Standard deviation of the array elements:")
std = x.std()
print(std)
print("Variance of the array elements:")
var = x.var()
print(var)

import pandas as pd
data = pd.read_csv('wine.csv')
data

data.describe()

minValuesObj = data.min()
print('minimum value in each column : ')
print(minValuesObj)

maxValuesObj = data.max()
print('maximum value in each column : ')
print(maxValuesObj)

data.mean(axis = 0)

import numpy as np
x = np.random.randint(low=10, high=30, size=6)
print(x)

import numpy as np
x = np.random.random((5,5))
print("Original Array:")
print(x) 
xmin, xmax = x.min(), x.max()
print("Minimum and Maximum Values:")
print(xmin, xmax)

import pandas as pd
data = pd.read_csv('iris.csv')
data

data.describe()

data.head()  # shows top n entries (default n = 5)

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt

data = pd.read_csv("iris.csv") 
  
print (data.head(10))

data.describe()

#Histogram for Sepal Length
plt.figure(figsize = (10, 7)) 
x = data["sepal_length"] 
  
plt.hist(x, bins = 20, color = "green") 
plt.title("Sepal Length in cm") 
plt.xlabel("Sepal_Length_cm") 
plt.ylabel("Count")

#Histogram for sepal width
plt.figure(figsize = (10, 7)) 
x = data.sepal_width 
  
plt.hist(x, bins = 20, color = "green") 
plt.title("Sepal Width in cm") 
plt.xlabel("Sepal_Width_cm") 
plt.ylabel("Count") 
  
plt.show()

#histogram for petal length
plt.figure(figsize = (10, 7)) 
x = data.petal_length 
  
plt.hist(x, bins = 20, color = "green") 
plt.title("Petal Length in cm") 
plt.xlabel("Petal_Length_cm") 
plt.ylabel("Count") 
  
plt.show()

#histogram for petal width
plt.figure(figsize = (10, 7)) 
x = data.petal_width 
  
plt.hist(x, bins = 20, color = "green") 
plt.title("Petal Width in cm") 
plt.xlabel("Petal_Width_cm") 
plt.ylabel("Count") 
  
plt.show()

# removing Id column 
new_data = data[["sepal_length", "sepal_width", "petal_length", "petal_width","plant"]] 
print(new_data.head())

plt.figure(figsize = (10, 7)) 
new_data.boxplot()

import matplotlib.pyplot as plt
import seaborn as sns
iris = sns.load_dataset("iris")
iris["ID"] = iris.index
iris["ratio"] = iris["sepal_length"]/iris["sepal_width"]

sns.lmplot(x="ID", y="ratio", data=iris, hue="species", fit_reg=False, legend=False)

plt.legend()
plt.show()

import numpy as np 
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# plt.style.use('default')
color_pallete = ['#fc5185', '#3fc1c9', '#364f6b']
sns.set_palette(color_pallete)
sns.set_style("white")

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

df = pd.read_csv('iris.csv')
df.head()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn import datasets
from sklearn.decomposition import PCA

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target

x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5

plt.figure(2, figsize=(8, 6))
plt.clf()

# Plot the training points
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,
            edgecolor='k')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())

# To getter a better understanding of interaction of the dimensions
# plot the first three PCA dimensions
fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)
X_reduced = PCA(n_components=3).fit_transform(iris.data)
ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,
           cmap=plt.cm.Set1, edgecolor='k', s=40)
ax.set_title("First three PCA directions")
ax.set_xlabel("1st eigenvector")
ax.w_xaxis.set_ticklabels([])
ax.set_ylabel("2nd eigenvector")
ax.w_yaxis.set_ticklabels([])
ax.set_zlabel("3rd eigenvector")
ax.w_zaxis.set_ticklabels([])

plt.show()

"""
Inference according to me,
By default, the model will assign the item to the class with the highest probability. 
If we wanted to adjust the accuracy or precision, we could do this by changing the threshold of how high the predicted probability would have to be before it was assigned to that class.
In this case, there is not a consequence to incorrectly assigning a flower to another class, 
but models used to detect cancer cells adjust their models to ‘assume the worst’ and assign it as a true cancer cell more often. 
"""